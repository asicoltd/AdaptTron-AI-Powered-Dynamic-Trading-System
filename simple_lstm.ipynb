{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.128786\n",
      "Epoch 10, Loss: 0.083989\n",
      "Epoch 20, Loss: 0.082453\n",
      "Epoch 30, Loss: 0.082530\n",
      "Epoch 40, Loss: 0.081862\n",
      "Epoch 50, Loss: 0.080511\n",
      "Epoch 60, Loss: 0.080161\n",
      "Epoch 70, Loss: 0.079151\n",
      "Epoch 80, Loss: 0.077884\n",
      "Epoch 90, Loss: 0.074994\n",
      "Epoch 100, Loss: 0.071784\n",
      "Epoch 110, Loss: 0.069278\n",
      "Epoch 120, Loss: 0.064713\n",
      "Epoch 130, Loss: 0.056952\n",
      "Epoch 140, Loss: 0.046053\n",
      "Epoch 150, Loss: 0.037014\n",
      "Epoch 160, Loss: 0.026891\n",
      "Epoch 170, Loss: 0.017303\n",
      "Epoch 180, Loss: 0.009546\n",
      "Epoch 190, Loss: 0.005055\n",
      "Epoch 200, Loss: 0.002908\n",
      "Epoch 210, Loss: 0.001872\n",
      "Epoch 220, Loss: 0.000728\n",
      "Epoch 230, Loss: 0.000711\n",
      "Epoch 240, Loss: 0.000227\n",
      "Epoch 250, Loss: 0.000291\n",
      "Epoch 260, Loss: 0.000582\n",
      "Epoch 270, Loss: 0.000956\n",
      "Epoch 280, Loss: 0.001853\n",
      "Epoch 290, Loss: 0.000745\n",
      "Epoch 300, Loss: 0.000364\n",
      "Epoch 310, Loss: 0.000659\n",
      "Epoch 320, Loss: 0.000768\n",
      "Epoch 330, Loss: 0.001130\n",
      "Epoch 340, Loss: 0.000419\n",
      "Epoch 350, Loss: 0.000725\n",
      "Epoch 360, Loss: 0.001270\n",
      "Epoch 370, Loss: 0.000258\n",
      "Epoch 380, Loss: 0.000314\n",
      "Epoch 390, Loss: 0.000705\n",
      "Epoch 400, Loss: 0.000571\n",
      "Epoch 410, Loss: 0.000633\n",
      "Epoch 420, Loss: 0.000557\n",
      "Epoch 430, Loss: 0.000685\n",
      "Epoch 440, Loss: 0.001309\n",
      "Epoch 450, Loss: 0.000117\n",
      "Epoch 460, Loss: 0.000054\n",
      "Epoch 470, Loss: 0.000152\n",
      "Epoch 480, Loss: 0.002029\n",
      "Epoch 490, Loss: 0.000142\n",
      "Epoch 500, Loss: 0.000081\n",
      "Epoch 510, Loss: 0.000439\n",
      "Epoch 520, Loss: 0.001183\n",
      "Epoch 530, Loss: 0.000351\n",
      "Epoch 540, Loss: 0.000161\n",
      "Epoch 550, Loss: 0.000366\n",
      "Epoch 560, Loss: 0.000774\n",
      "Epoch 570, Loss: 0.000821\n",
      "Epoch 580, Loss: 0.000261\n",
      "Epoch 590, Loss: 0.000179\n",
      "Epoch 600, Loss: 0.000411\n",
      "Epoch 610, Loss: 0.000845\n",
      "Epoch 620, Loss: 0.000551\n",
      "Epoch 630, Loss: 0.000131\n",
      "Epoch 640, Loss: 0.000196\n",
      "Epoch 650, Loss: 0.001157\n",
      "Epoch 660, Loss: 0.000538\n",
      "Epoch 670, Loss: 0.000085\n",
      "Epoch 680, Loss: 0.000183\n",
      "Epoch 690, Loss: 0.000651\n",
      "Epoch 700, Loss: 0.000226\n",
      "Epoch 710, Loss: 0.000049\n",
      "Epoch 720, Loss: 0.000111\n",
      "Epoch 730, Loss: 0.000690\n",
      "Epoch 740, Loss: 0.000569\n",
      "Epoch 750, Loss: 0.000254\n",
      "Epoch 760, Loss: 0.000142\n",
      "Epoch 770, Loss: 0.000274\n",
      "Epoch 780, Loss: 0.000629\n",
      "Epoch 790, Loss: 0.000224\n",
      "Epoch 800, Loss: 0.000183\n",
      "Epoch 810, Loss: 0.000309\n",
      "Epoch 820, Loss: 0.000676\n",
      "Epoch 830, Loss: 0.000142\n",
      "Epoch 840, Loss: 0.000143\n",
      "Epoch 850, Loss: 0.000326\n",
      "Epoch 860, Loss: 0.000386\n",
      "Epoch 870, Loss: 0.000180\n",
      "Epoch 880, Loss: 0.000467\n",
      "Epoch 890, Loss: 0.000199\n",
      "Epoch 900, Loss: 0.000159\n",
      "Epoch 910, Loss: 0.000226\n",
      "Epoch 920, Loss: 0.000387\n",
      "Epoch 930, Loss: 0.000233\n",
      "Epoch 940, Loss: 0.000328\n",
      "Epoch 950, Loss: 0.000293\n",
      "Epoch 960, Loss: 0.000150\n",
      "Epoch 970, Loss: 0.000408\n",
      "Epoch 980, Loss: 0.000456\n",
      "Epoch 990, Loss: 0.000133\n",
      "Epoch 1000, Loss: 0.000264\n",
      "Epoch 1010, Loss: 0.000109\n",
      "Epoch 1020, Loss: 0.000249\n",
      "Epoch 1030, Loss: 0.000304\n",
      "Epoch 1040, Loss: 0.000433\n",
      "Epoch 1050, Loss: 0.000239\n",
      "Epoch 1060, Loss: 0.000279\n",
      "Epoch 1070, Loss: 0.000064\n",
      "Epoch 1080, Loss: 0.000081\n",
      "Epoch 1090, Loss: 0.000370\n",
      "Epoch 1100, Loss: 0.000196\n",
      "Epoch 1110, Loss: 0.000179\n",
      "Epoch 1120, Loss: 0.000261\n",
      "Epoch 1130, Loss: 0.000247\n",
      "Epoch 1140, Loss: 0.000119\n",
      "Epoch 1150, Loss: 0.000143\n",
      "Epoch 1160, Loss: 0.000202\n",
      "Epoch 1170, Loss: 0.000194\n",
      "Epoch 1180, Loss: 0.000307\n",
      "Epoch 1190, Loss: 0.000164\n",
      "Epoch 1200, Loss: 0.000157\n",
      "Epoch 1210, Loss: 0.000162\n",
      "Epoch 1220, Loss: 0.000207\n",
      "Epoch 1230, Loss: 0.000176\n",
      "Epoch 1240, Loss: 0.000315\n",
      "Epoch 1250, Loss: 0.000137\n",
      "Epoch 1260, Loss: 0.000047\n",
      "Epoch 1270, Loss: 0.000048\n",
      "Epoch 1280, Loss: 0.000386\n",
      "Epoch 1290, Loss: 0.000215\n",
      "Epoch 1300, Loss: 0.000221\n",
      "Epoch 1310, Loss: 0.000106\n",
      "Epoch 1320, Loss: 0.000107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 85\u001b[0m\n\u001b[0;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[0;32m     84\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 85\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     88\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ============================\n",
    "# 1️⃣ Data Preparation\n",
    "# ============================\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 5  # Number of previous candlesticks\n",
    "FEATURES = 5  # (Open, High, Low, Close, Volume)\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Simulated dataset (Replace with real data)\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(1000, FEATURES)  # Simulating 1000 candlesticks (OHLCV)\n",
    "\n",
    "# Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert data into sequences\n",
    "X, y = [], []\n",
    "for i in range(len(data) - SEQ_LEN):\n",
    "    X.append(data[i:i + SEQ_LEN])\n",
    "    y.append(data[i + SEQ_LEN][3])  # Predicting next Close price\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split Data (Train: 80%, Test: 20%)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).view(-1, 1), torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ LSTM Model\n",
    "# ============================\n",
    "class CandlestickLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CandlestickLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=FEATURES, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, 1)  # Predict next Close price\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Take last time step's output\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "model = CandlestickLSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ Training Loop + Loss Plot\n",
    "# ============================\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {avg_loss:.6f}')\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Evaluation on Test Data\n",
    "# ============================\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = model(X_batch)\n",
    "        y_true.extend(y_batch.numpy().flatten())\n",
    "        y_pred.extend(predictions.numpy().flatten())\n",
    "\n",
    "# Convert Predictions Back to Original Scale\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Plot Predictions vs. Actual\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_true, label=\"Actual Close Price\", color='blue', alpha=0.7)\n",
    "plt.plot(y_pred, label=\"Predicted Close Price\", color='red', linestyle=\"dashed\", alpha=0.8)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.title(\"Predicted vs. Actual Close Price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ Model Performance Metrics\n",
    "# ============================\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"📌 Model Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}\")\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ Prediction Function\n",
    "# ============================\n",
    "def predict_next_close(model, last_5_candles):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_data = torch.tensor(last_5_candles, dtype=torch.float32).unsqueeze(0)\n",
    "        return model(input_data).item()\n",
    "\n",
    "# Example Prediction\n",
    "sample_input = data[-5:]  # Last 5 candlesticks\n",
    "predicted_close = predict_next_close(model, sample_input)\n",
    "print(\"Predicted Next Close Price:\", predicted_close)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding more indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3444495392\n",
      "Epoch 10, Loss: 0.3470057267\n",
      "Epoch 20, Loss: 0.3399769104\n",
      "Epoch 30, Loss: 0.3450610167\n",
      "Epoch 40, Loss: 0.3358251286\n",
      "Epoch 50, Loss: 0.3404396689\n",
      "Epoch 60, Loss: 0.3424739754\n",
      "Epoch 70, Loss: 0.3405193228\n",
      "Epoch 80, Loss: 0.3397761816\n",
      "Epoch 90, Loss: 0.3412737608\n",
      "Epoch 100, Loss: 0.3430178893\n",
      "Epoch 110, Loss: 0.3432188410\n",
      "Epoch 120, Loss: 0.3470231128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m    131\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[1;32m--> 132\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    134\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ============================\n",
    "# 1️⃣ Technical Indicator Functions\n",
    "# ============================\n",
    "\n",
    "def compute_RSI(data, window=14):\n",
    "    delta = np.diff(data)\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "\n",
    "    avg_gain = np.convolve(gain, np.ones(window)/window, mode='valid')\n",
    "    avg_loss = np.convolve(loss, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    rs = avg_gain / (avg_loss + 1e-9)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return np.concatenate((np.full(window, np.nan), rsi))  # Pad with NaNs for alignment\n",
    "\n",
    "def compute_MACD(data, short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = pd.Series(data).ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = pd.Series(data).ewm(span=long_window, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "    \n",
    "    return macd.values, signal.values\n",
    "\n",
    "def compute_Bollinger_Bands(data, window=20, num_std=2):\n",
    "    rolling_mean = pd.Series(data).rolling(window=window).mean()\n",
    "    rolling_std = pd.Series(data).rolling(window=window).std()\n",
    "\n",
    "    upper_band = rolling_mean + (num_std * rolling_std)\n",
    "    lower_band = rolling_mean - (num_std * rolling_std)\n",
    "    \n",
    "    return rolling_mean.values, upper_band.values, lower_band.values\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Data Preparation\n",
    "# ============================\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 5\n",
    "FEATURES = 8  # OHLCV + RSI + MACD + Bollinger Bands\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 32\n",
    "LEARNING_RATE = 0.0000001\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Simulated dataset (Replace with real data)\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(1000, 5)  # Simulating 1000 candlesticks (OHLCV)\n",
    "\n",
    "# Compute Indicators\n",
    "close_prices = data[:, 3]  # Extract Close prices\n",
    "\n",
    "rsi = compute_RSI(close_prices)\n",
    "macd, macd_signal = compute_MACD(close_prices)\n",
    "bb_middle, bb_upper, bb_lower = compute_Bollinger_Bands(close_prices)\n",
    "\n",
    "# Align and merge features\n",
    "data = np.column_stack((data, rsi, macd, bb_middle))  # Merging RSI, MACD, Bollinger Bands\n",
    "\n",
    "# Remove NaN rows (due to indicators)\n",
    "data = data[30:]  # Remove first 30 rows (max required for MACD/Bollinger Bands)\n",
    "\n",
    "# Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert data into sequences\n",
    "X, y = [], []\n",
    "for i in range(len(data) - SEQ_LEN):\n",
    "    X.append(data[i:i + SEQ_LEN])\n",
    "    y.append(data[i + SEQ_LEN][3])  # Predict next Close price\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split Data (Train: 80%, Test: 20%)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).view(-1, 1), torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ LSTM Model\n",
    "# ============================\n",
    "class CandlestickLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CandlestickLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=FEATURES, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, 1)  # Predict next Close price\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Take last time step's output\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "model = CandlestickLSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Training Loop + Loss Plot\n",
    "# ============================\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {avg_loss:.10f}')\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ Evaluation on Test Data\n",
    "# ============================\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = model(X_batch)\n",
    "        y_true.extend(y_batch.numpy().flatten())\n",
    "        y_pred.extend(predictions.numpy().flatten())\n",
    "\n",
    "# Convert Predictions Back to Original Scale\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Plot Predictions vs. Actual\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_true, label=\"Actual Close Price\", color='blue', alpha=0.7)\n",
    "plt.plot(y_pred, label=\"Predicted Close Price\", color='red', linestyle=\"dashed\", alpha=0.8)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.title(\"Predicted vs. Actual Close Price with Indicators\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ Model Performance Metrics\n",
    "# ============================\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"📌 Model Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}\")\n",
    "\n",
    "# ============================\n",
    "# 7️⃣ Prediction Function\n",
    "# ============================\n",
    "def predict_next_close(model, last_5_candles):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_data = torch.tensor(last_5_candles, dtype=torch.float32).unsqueeze(0)\n",
    "        return model(input_data).item()\n",
    "\n",
    "# Example Prediction\n",
    "sample_input = data[-5:]  # Last 5 candlesticks\n",
    "predicted_close = predict_next_close(model, sample_input)\n",
    "print(\"Predicted Next Close Price:\", predicted_close)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0, Loss: 0.1934572201\n",
      "Epoch 10, Loss: 0.1608726546\n",
      "Epoch 20, Loss: 0.1213262624\n",
      "Epoch 30, Loss: 0.0923125823\n",
      "Epoch 40, Loss: 0.0844411919\n",
      "Epoch 50, Loss: 0.0849650574\n",
      "Epoch 60, Loss: 0.0842214210\n",
      "Epoch 70, Loss: 0.0833795914\n",
      "Epoch 80, Loss: 0.0835696551\n",
      "Epoch 90, Loss: 0.0823718078\n",
      "Epoch 100, Loss: 0.0842141157\n",
      "Epoch 110, Loss: 0.0853668532\n",
      "Epoch 120, Loss: 0.0844085881\n",
      "Epoch 130, Loss: 0.0816264265\n",
      "Epoch 140, Loss: 0.0821522215\n",
      "Epoch 150, Loss: 0.0837482335\n",
      "Epoch 160, Loss: 0.0824481919\n",
      "Epoch 170, Loss: 0.0812092628\n",
      "Epoch 180, Loss: 0.0810058554\n",
      "Epoch 190, Loss: 0.0821929748\n",
      "Epoch 200, Loss: 0.0834310687\n",
      "Epoch 210, Loss: 0.0836448637\n",
      "Epoch 220, Loss: 0.0824503243\n",
      "Epoch 230, Loss: 0.0838371187\n",
      "Epoch 240, Loss: 0.0828663249\n",
      "Epoch 250, Loss: 0.0833668207\n",
      "Epoch 260, Loss: 0.0832002354\n",
      "Epoch 270, Loss: 0.0812972362\n",
      "Epoch 280, Loss: 0.0806283715\n",
      "Epoch 290, Loss: 0.0817981836\n",
      "Epoch 300, Loss: 0.0823424095\n",
      "Epoch 310, Loss: 0.0824819314\n",
      "Epoch 320, Loss: 0.0835978228\n",
      "Epoch 330, Loss: 0.0837857291\n",
      "Epoch 340, Loss: 0.0809198618\n",
      "Epoch 350, Loss: 0.0818332276\n",
      "Epoch 360, Loss: 0.0838575177\n",
      "Epoch 370, Loss: 0.0818851323\n",
      "Epoch 380, Loss: 0.0798716860\n",
      "Epoch 390, Loss: 0.0818955815\n",
      "Epoch 400, Loss: 0.0804428914\n",
      "Epoch 410, Loss: 0.0817326701\n",
      "Epoch 420, Loss: 0.0817829214\n",
      "Epoch 430, Loss: 0.0797811420\n",
      "Epoch 440, Loss: 0.0801181696\n",
      "Epoch 450, Loss: 0.0823820707\n",
      "Epoch 460, Loss: 0.0835844122\n",
      "Epoch 470, Loss: 0.0818803924\n",
      "Epoch 480, Loss: 0.0808717257\n",
      "Epoch 490, Loss: 0.0841705704\n",
      "Epoch 500, Loss: 0.0801328050\n",
      "Epoch 510, Loss: 0.0824071762\n",
      "Epoch 520, Loss: 0.0824651286\n",
      "Epoch 530, Loss: 0.0819300899\n",
      "Epoch 540, Loss: 0.0794224150\n",
      "Epoch 550, Loss: 0.0812638254\n",
      "Epoch 560, Loss: 0.0797830960\n",
      "Epoch 570, Loss: 0.0799683402\n",
      "Epoch 580, Loss: 0.0831466122\n",
      "Epoch 590, Loss: 0.0826307353\n",
      "Epoch 600, Loss: 0.0820281264\n",
      "Epoch 610, Loss: 0.0837107158\n",
      "Epoch 620, Loss: 0.0805939654\n",
      "Epoch 630, Loss: 0.0810370311\n",
      "Epoch 640, Loss: 0.0808686742\n",
      "Epoch 650, Loss: 0.0818632144\n",
      "Epoch 660, Loss: 0.0791827156\n",
      "Epoch 670, Loss: 0.0804574589\n",
      "Epoch 680, Loss: 0.0787365538\n",
      "Epoch 690, Loss: 0.0813556500\n",
      "Epoch 700, Loss: 0.0821342644\n",
      "Epoch 710, Loss: 0.0831239858\n",
      "Epoch 720, Loss: 0.0794604634\n",
      "Epoch 730, Loss: 0.0811417332\n",
      "Epoch 740, Loss: 0.0817894605\n",
      "Epoch 750, Loss: 0.0804716259\n",
      "Epoch 760, Loss: 0.0813665932\n",
      "Epoch 770, Loss: 0.0800699255\n",
      "Epoch 780, Loss: 0.0810557874\n",
      "Epoch 790, Loss: 0.0814865258\n",
      "Epoch 800, Loss: 0.0810372010\n",
      "Epoch 810, Loss: 0.0805262837\n",
      "Epoch 820, Loss: 0.0813043886\n",
      "Epoch 830, Loss: 0.0822356471\n",
      "Epoch 840, Loss: 0.0805100219\n",
      "Epoch 850, Loss: 0.0798751108\n",
      "Epoch 860, Loss: 0.0807830606\n",
      "Epoch 870, Loss: 0.0803262576\n",
      "Epoch 880, Loss: 0.0839812022\n",
      "Epoch 890, Loss: 0.0804521242\n",
      "Epoch 900, Loss: 0.0815129977\n",
      "Epoch 910, Loss: 0.0797411415\n",
      "Epoch 920, Loss: 0.0800662768\n",
      "Epoch 930, Loss: 0.0815912548\n",
      "Epoch 940, Loss: 0.0799311309\n",
      "Epoch 950, Loss: 0.0797490333\n",
      "Epoch 960, Loss: 0.0813302496\n",
      "Epoch 970, Loss: 0.0801191635\n",
      "Epoch 980, Loss: 0.0786086053\n",
      "Epoch 990, Loss: 0.0819723994\n",
      "Epoch 1000, Loss: 0.0806291749\n",
      "Epoch 1010, Loss: 0.0799850373\n",
      "Epoch 1020, Loss: 0.0839381059\n",
      "Epoch 1030, Loss: 0.0796350713\n",
      "Epoch 1040, Loss: 0.0812530221\n",
      "Epoch 1050, Loss: 0.0809348860\n",
      "Epoch 1060, Loss: 0.0823065554\n",
      "Epoch 1070, Loss: 0.0811256132\n",
      "Epoch 1080, Loss: 0.0792610577\n",
      "Epoch 1090, Loss: 0.0790045467\n",
      "Epoch 1100, Loss: 0.0805633710\n",
      "Epoch 1110, Loss: 0.0803587915\n",
      "Epoch 1120, Loss: 0.0810738251\n",
      "Epoch 1130, Loss: 0.0791786052\n",
      "Epoch 1140, Loss: 0.0813076888\n",
      "Epoch 1150, Loss: 0.0792000279\n",
      "Epoch 1160, Loss: 0.0802045259\n",
      "Epoch 1170, Loss: 0.0800444846\n",
      "Epoch 1180, Loss: 0.0808033186\n",
      "Epoch 1190, Loss: 0.0783370231\n",
      "Epoch 1200, Loss: 0.0798946998\n",
      "Epoch 1210, Loss: 0.0810106283\n",
      "Epoch 1220, Loss: 0.0804033294\n",
      "Epoch 1230, Loss: 0.0816096966\n",
      "Epoch 1240, Loss: 0.0803061679\n",
      "Epoch 1250, Loss: 0.0799090303\n",
      "Epoch 1260, Loss: 0.0831139985\n",
      "Epoch 1270, Loss: 0.0798903573\n",
      "Epoch 1280, Loss: 0.0792544307\n",
      "Epoch 1290, Loss: 0.0801005715\n",
      "Epoch 1300, Loss: 0.0809851825\n",
      "Epoch 1310, Loss: 0.0798035784\n",
      "Epoch 1320, Loss: 0.0794158165\n",
      "Epoch 1330, Loss: 0.0823030347\n",
      "Epoch 1340, Loss: 0.0806488594\n",
      "Epoch 1350, Loss: 0.0795952480\n",
      "Epoch 1360, Loss: 0.0815512495\n",
      "Epoch 1370, Loss: 0.0805421594\n",
      "Epoch 1380, Loss: 0.0789796850\n",
      "Epoch 1390, Loss: 0.0792034820\n",
      "Epoch 1400, Loss: 0.0817857447\n",
      "Epoch 1410, Loss: 0.0823140544\n",
      "Epoch 1420, Loss: 0.0806699139\n",
      "Epoch 1430, Loss: 0.0799938558\n",
      "Epoch 1440, Loss: 0.0793761124\n",
      "Epoch 1450, Loss: 0.0846069996\n",
      "Epoch 1460, Loss: 0.0787831813\n",
      "Epoch 1470, Loss: 0.0819770861\n",
      "Epoch 1480, Loss: 0.0817076297\n",
      "Epoch 1490, Loss: 0.0830526333\n",
      "Epoch 1500, Loss: 0.0791809392\n",
      "Epoch 1510, Loss: 0.0787714642\n",
      "Epoch 1520, Loss: 0.0817659965\n",
      "Epoch 1530, Loss: 0.0801363581\n",
      "Epoch 1540, Loss: 0.0799079856\n",
      "Epoch 1550, Loss: 0.0816957594\n",
      "Epoch 1560, Loss: 0.0798749660\n",
      "Epoch 1570, Loss: 0.0801079983\n",
      "Epoch 1580, Loss: 0.0844376525\n",
      "Epoch 1590, Loss: 0.0792566973\n",
      "Epoch 1600, Loss: 0.0806122252\n",
      "Epoch 1610, Loss: 0.0792634210\n",
      "Epoch 1620, Loss: 0.0790148222\n",
      "Epoch 1630, Loss: 0.0803015736\n",
      "Epoch 1640, Loss: 0.0814820471\n",
      "Epoch 1650, Loss: 0.0807419415\n",
      "Epoch 1660, Loss: 0.0821828131\n",
      "Epoch 1670, Loss: 0.0796300456\n",
      "Epoch 1680, Loss: 0.0789777535\n",
      "Epoch 1690, Loss: 0.0807667705\n",
      "Epoch 1700, Loss: 0.0821980184\n",
      "Epoch 1710, Loss: 0.0813055597\n",
      "Epoch 1720, Loss: 0.0821934484\n",
      "Epoch 1730, Loss: 0.0813986507\n",
      "Epoch 1740, Loss: 0.0781753089\n",
      "Epoch 1750, Loss: 0.0794458278\n",
      "Epoch 1760, Loss: 0.0802061564\n",
      "Epoch 1770, Loss: 0.0801611888\n",
      "Epoch 1780, Loss: 0.0797563407\n",
      "Epoch 1790, Loss: 0.0810664092\n",
      "Epoch 1800, Loss: 0.0808792579\n",
      "Epoch 1810, Loss: 0.0804888014\n",
      "Epoch 1820, Loss: 0.0802498785\n",
      "Epoch 1830, Loss: 0.0816391367\n",
      "Epoch 1840, Loss: 0.0808002615\n",
      "Epoch 1850, Loss: 0.0813231990\n",
      "Epoch 1860, Loss: 0.0791920066\n",
      "Epoch 1870, Loss: 0.0801724851\n",
      "Epoch 1880, Loss: 0.0787282957\n",
      "Epoch 1890, Loss: 0.0805227694\n",
      "Epoch 1900, Loss: 0.0794664535\n",
      "Epoch 1910, Loss: 0.0779389872\n",
      "Epoch 1920, Loss: 0.0788651778\n",
      "Epoch 1930, Loss: 0.0814426933\n",
      "Epoch 1940, Loss: 0.0795592576\n",
      "Epoch 1950, Loss: 0.0813648283\n",
      "Epoch 1960, Loss: 0.0778068163\n",
      "Epoch 1970, Loss: 0.0806329538\n",
      "Epoch 1980, Loss: 0.0794305538\n",
      "Epoch 1990, Loss: 0.0792287755\n",
      "Epoch 2000, Loss: 0.0787276906\n",
      "Epoch 2010, Loss: 0.0799165744\n",
      "Epoch 2020, Loss: 0.0797593313\n",
      "Epoch 2030, Loss: 0.0829387829\n",
      "Epoch 2040, Loss: 0.0806168222\n",
      "Epoch 2050, Loss: 0.0805164513\n",
      "Epoch 2060, Loss: 0.0787620294\n",
      "Epoch 2070, Loss: 0.0814140978\n",
      "Epoch 2080, Loss: 0.0796596923\n",
      "Epoch 2090, Loss: 0.0824876961\n",
      "Epoch 2100, Loss: 0.0802902558\n",
      "Epoch 2110, Loss: 0.0819372761\n",
      "Epoch 2120, Loss: 0.0807217331\n",
      "Epoch 2130, Loss: 0.0793544900\n",
      "Epoch 2140, Loss: 0.0797727467\n",
      "Epoch 2150, Loss: 0.0789677475\n",
      "Epoch 2160, Loss: 0.0822394405\n",
      "Epoch 2170, Loss: 0.0795216292\n",
      "Epoch 2180, Loss: 0.0782872109\n",
      "Epoch 2190, Loss: 0.0803376386\n",
      "Epoch 2200, Loss: 0.0801093712\n",
      "Epoch 2210, Loss: 0.0816466385\n",
      "Epoch 2220, Loss: 0.0789350504\n",
      "Epoch 2230, Loss: 0.0819052911\n",
      "Epoch 2240, Loss: 0.0805621493\n",
      "Epoch 2250, Loss: 0.0818327400\n",
      "Epoch 2260, Loss: 0.0792052582\n",
      "Epoch 2270, Loss: 0.0788882184\n",
      "Epoch 2280, Loss: 0.0800011602\n",
      "Epoch 2290, Loss: 0.0799711360\n",
      "Epoch 2300, Loss: 0.0800945109\n",
      "Epoch 2310, Loss: 0.0814223740\n",
      "Epoch 2320, Loss: 0.0791058895\n",
      "Epoch 2330, Loss: 0.0780919299\n",
      "Epoch 2340, Loss: 0.0836991425\n",
      "Epoch 2350, Loss: 0.0806000534\n",
      "Epoch 2360, Loss: 0.0778802537\n",
      "Epoch 2370, Loss: 0.0820960481\n",
      "Epoch 2380, Loss: 0.0787664135\n",
      "Epoch 2390, Loss: 0.0785634954\n",
      "Epoch 2400, Loss: 0.0778756054\n",
      "Epoch 2410, Loss: 0.0802260430\n",
      "Epoch 2420, Loss: 0.0802764969\n",
      "Epoch 2430, Loss: 0.0797818947\n",
      "Epoch 2440, Loss: 0.0789611082\n",
      "Epoch 2450, Loss: 0.0795128714\n",
      "Epoch 2460, Loss: 0.0788861567\n",
      "Epoch 2470, Loss: 0.0795961046\n",
      "Epoch 2480, Loss: 0.0797022486\n",
      "Epoch 2490, Loss: 0.0820041366\n",
      "Epoch 2500, Loss: 0.0818636127\n",
      "Epoch 2510, Loss: 0.0804372416\n",
      "Epoch 2520, Loss: 0.0789557894\n",
      "Epoch 2530, Loss: 0.0796829009\n",
      "Epoch 2540, Loss: 0.0808504187\n",
      "Epoch 2550, Loss: 0.0820705931\n",
      "Epoch 2560, Loss: 0.0795125148\n",
      "Epoch 2570, Loss: 0.0787542599\n",
      "Epoch 2580, Loss: 0.0824233522\n",
      "Epoch 2590, Loss: 0.0776703642\n",
      "Epoch 2600, Loss: 0.0785018392\n",
      "Epoch 2610, Loss: 0.0800492412\n",
      "Epoch 2620, Loss: 0.0806758627\n",
      "Epoch 2630, Loss: 0.0780526333\n",
      "Epoch 2640, Loss: 0.0780989366\n",
      "Epoch 2650, Loss: 0.0781334171\n",
      "Epoch 2660, Loss: 0.0821386433\n",
      "Epoch 2670, Loss: 0.0796098167\n",
      "Epoch 2680, Loss: 0.0776079894\n",
      "Epoch 2690, Loss: 0.0792052807\n",
      "Epoch 2700, Loss: 0.0783041878\n",
      "Epoch 2710, Loss: 0.0789255911\n",
      "Epoch 2720, Loss: 0.0781058474\n",
      "Epoch 2730, Loss: 0.0804102412\n",
      "Epoch 2740, Loss: 0.0806360584\n",
      "Epoch 2750, Loss: 0.0807419570\n",
      "Epoch 2760, Loss: 0.0814771906\n",
      "Epoch 2770, Loss: 0.0799163461\n",
      "Epoch 2780, Loss: 0.0801611772\n",
      "Epoch 2790, Loss: 0.0810598852\n",
      "Epoch 2800, Loss: 0.0783182222\n",
      "Epoch 2810, Loss: 0.0812837148\n",
      "Epoch 2820, Loss: 0.0792147037\n",
      "Epoch 2830, Loss: 0.0801494956\n",
      "Epoch 2840, Loss: 0.0789393570\n",
      "Epoch 2850, Loss: 0.0797902900\n",
      "Epoch 2860, Loss: 0.0780554957\n",
      "Epoch 2870, Loss: 0.0809150977\n",
      "Epoch 2880, Loss: 0.0784946877\n",
      "Epoch 2890, Loss: 0.0776764418\n",
      "Epoch 2900, Loss: 0.0800206602\n",
      "Epoch 2910, Loss: 0.0798196799\n",
      "Epoch 2920, Loss: 0.0773479193\n",
      "Epoch 2930, Loss: 0.0793340223\n",
      "Epoch 2940, Loss: 0.0802153058\n",
      "Epoch 2950, Loss: 0.0799977010\n",
      "Epoch 2960, Loss: 0.0797625881\n",
      "Epoch 2970, Loss: 0.0798389326\n",
      "Epoch 2980, Loss: 0.0807880609\n",
      "Epoch 2990, Loss: 0.0807091959\n",
      "Epoch 3000, Loss: 0.0801395723\n",
      "Epoch 3010, Loss: 0.0773026287\n",
      "Epoch 3020, Loss: 0.0784816681\n",
      "Epoch 3030, Loss: 0.0810222134\n",
      "Epoch 3040, Loss: 0.0807445209\n",
      "Epoch 3050, Loss: 0.0792831200\n",
      "Epoch 3060, Loss: 0.0817528802\n",
      "Epoch 3070, Loss: 0.0806905776\n",
      "Epoch 3080, Loss: 0.0798405772\n",
      "Epoch 3090, Loss: 0.0788636199\n",
      "Epoch 3100, Loss: 0.0782629618\n",
      "Epoch 3110, Loss: 0.0775346831\n",
      "Epoch 3120, Loss: 0.0796281260\n",
      "Epoch 3130, Loss: 0.0790405300\n",
      "Epoch 3140, Loss: 0.0802793188\n",
      "Epoch 3150, Loss: 0.0779160696\n",
      "Epoch 3160, Loss: 0.0813482574\n",
      "Epoch 3170, Loss: 0.0808795580\n",
      "Epoch 3180, Loss: 0.0804486874\n",
      "Epoch 3190, Loss: 0.0789197867\n",
      "Epoch 3200, Loss: 0.0814221498\n",
      "Epoch 3210, Loss: 0.0800409171\n",
      "Epoch 3220, Loss: 0.0792055319\n",
      "Epoch 3230, Loss: 0.0774706388\n",
      "Epoch 3240, Loss: 0.0800080447\n",
      "Epoch 3250, Loss: 0.0785795268\n",
      "Epoch 3260, Loss: 0.0794040525\n",
      "Epoch 3270, Loss: 0.0799222185\n",
      "Epoch 3280, Loss: 0.0787390962\n",
      "Epoch 3290, Loss: 0.0817485282\n",
      "Epoch 3300, Loss: 0.0796105111\n",
      "Epoch 3310, Loss: 0.0781839611\n",
      "Epoch 3320, Loss: 0.0785404693\n",
      "Epoch 3330, Loss: 0.0809350397\n",
      "Epoch 3340, Loss: 0.0785562445\n",
      "Epoch 3350, Loss: 0.0803029653\n",
      "Epoch 3360, Loss: 0.0798299082\n",
      "Epoch 3370, Loss: 0.0780548730\n",
      "Epoch 3380, Loss: 0.0819575509\n",
      "Epoch 3390, Loss: 0.0811748604\n",
      "Epoch 3400, Loss: 0.0801353703\n",
      "Epoch 3410, Loss: 0.0811213839\n",
      "Epoch 3420, Loss: 0.0788777848\n",
      "Epoch 3430, Loss: 0.0783680049\n",
      "Epoch 3440, Loss: 0.0787237313\n",
      "Epoch 3450, Loss: 0.0811185884\n",
      "Epoch 3460, Loss: 0.0800862934\n",
      "Epoch 3470, Loss: 0.0805368862\n",
      "Epoch 3480, Loss: 0.0771053383\n",
      "Epoch 3490, Loss: 0.0809740528\n",
      "Epoch 3500, Loss: 0.0794477153\n",
      "Epoch 3510, Loss: 0.0790140291\n",
      "Epoch 3520, Loss: 0.0797403516\n",
      "Epoch 3530, Loss: 0.0808077198\n",
      "Epoch 3540, Loss: 0.0788295937\n",
      "Epoch 3550, Loss: 0.0793803255\n",
      "Epoch 3560, Loss: 0.0773165550\n",
      "Epoch 3570, Loss: 0.0799461040\n",
      "Epoch 3580, Loss: 0.0796147090\n",
      "Epoch 3590, Loss: 0.0799799350\n",
      "Epoch 3600, Loss: 0.0808671442\n",
      "Epoch 3610, Loss: 0.0798657453\n",
      "Epoch 3620, Loss: 0.0807607438\n",
      "Epoch 3630, Loss: 0.0785877639\n",
      "Epoch 3640, Loss: 0.0803190392\n",
      "Epoch 3650, Loss: 0.0790045971\n",
      "Epoch 3660, Loss: 0.0786943436\n",
      "Epoch 3670, Loss: 0.0802080853\n",
      "Epoch 3680, Loss: 0.0806687255\n",
      "Epoch 3690, Loss: 0.0796258649\n",
      "Epoch 3700, Loss: 0.0819513817\n",
      "Epoch 3710, Loss: 0.0776853627\n",
      "Epoch 3720, Loss: 0.0783494532\n",
      "Epoch 3730, Loss: 0.0775637493\n",
      "Epoch 3740, Loss: 0.0776426065\n",
      "Epoch 3750, Loss: 0.0794323069\n",
      "Epoch 3760, Loss: 0.0787745258\n",
      "Epoch 3770, Loss: 0.0778514075\n",
      "Epoch 3780, Loss: 0.0783562073\n",
      "Epoch 3790, Loss: 0.0795453778\n",
      "Epoch 3800, Loss: 0.0789487797\n",
      "Epoch 3810, Loss: 0.0787728128\n",
      "Epoch 3820, Loss: 0.0777704315\n",
      "Epoch 3830, Loss: 0.0814679559\n",
      "Epoch 3840, Loss: 0.0796650355\n",
      "Epoch 3850, Loss: 0.0793267109\n",
      "Epoch 3860, Loss: 0.0796652192\n",
      "Epoch 3870, Loss: 0.0802624176\n",
      "Epoch 3880, Loss: 0.0790704080\n",
      "Epoch 3890, Loss: 0.0783433765\n",
      "Epoch 3900, Loss: 0.0835746866\n",
      "Epoch 3910, Loss: 0.0810999599\n",
      "Epoch 3920, Loss: 0.0804383697\n",
      "Epoch 3930, Loss: 0.0799431595\n",
      "Epoch 3940, Loss: 0.0798328555\n",
      "Epoch 3950, Loss: 0.0818448175\n",
      "Epoch 3960, Loss: 0.0795450112\n",
      "Epoch 3970, Loss: 0.0777475016\n",
      "Epoch 3980, Loss: 0.0821162476\n",
      "Epoch 3990, Loss: 0.0795866470\n",
      "Epoch 4000, Loss: 0.0777804577\n",
      "Epoch 4010, Loss: 0.0812417880\n",
      "Epoch 4020, Loss: 0.0786007987\n",
      "Epoch 4030, Loss: 0.0795940857\n",
      "Epoch 4040, Loss: 0.0778331938\n",
      "Epoch 4050, Loss: 0.0787410608\n",
      "Epoch 4060, Loss: 0.0784623364\n",
      "Epoch 4070, Loss: 0.0775492863\n",
      "Epoch 4080, Loss: 0.0792445561\n",
      "Epoch 4090, Loss: 0.0797638862\n",
      "Epoch 4100, Loss: 0.0781264539\n",
      "Epoch 4110, Loss: 0.0789899489\n",
      "Epoch 4120, Loss: 0.0806451888\n",
      "Epoch 4130, Loss: 0.0792287360\n",
      "Epoch 4140, Loss: 0.0779563314\n",
      "Epoch 4150, Loss: 0.0814743300\n",
      "Epoch 4160, Loss: 0.0788281360\n",
      "Epoch 4170, Loss: 0.0798614180\n",
      "Epoch 4180, Loss: 0.0784456843\n",
      "Epoch 4190, Loss: 0.0800326258\n",
      "Epoch 4200, Loss: 0.0802015868\n",
      "Epoch 4210, Loss: 0.0792031907\n",
      "Epoch 4220, Loss: 0.0787333640\n",
      "Epoch 4230, Loss: 0.0781405739\n",
      "Epoch 4240, Loss: 0.0805567375\n",
      "Epoch 4250, Loss: 0.0777759869\n",
      "Epoch 4260, Loss: 0.0770001584\n",
      "Epoch 4270, Loss: 0.0782507722\n",
      "Epoch 4280, Loss: 0.0784610331\n",
      "Epoch 4290, Loss: 0.0779224175\n",
      "Epoch 4300, Loss: 0.0776668978\n",
      "Epoch 4310, Loss: 0.0807465488\n",
      "Epoch 4320, Loss: 0.0800694653\n",
      "Epoch 4330, Loss: 0.0786985853\n",
      "Epoch 4340, Loss: 0.0799871273\n",
      "Epoch 4350, Loss: 0.0768670873\n",
      "Epoch 4360, Loss: 0.0777396473\n",
      "Epoch 4370, Loss: 0.0772035471\n",
      "Epoch 4380, Loss: 0.0792217307\n",
      "Epoch 4390, Loss: 0.0781712680\n",
      "Epoch 4400, Loss: 0.0800580904\n",
      "Epoch 4410, Loss: 0.0796196333\n",
      "Epoch 4420, Loss: 0.0783505322\n",
      "Epoch 4430, Loss: 0.0773385803\n",
      "Epoch 4440, Loss: 0.0774405146\n",
      "Epoch 4450, Loss: 0.0798320389\n",
      "Epoch 4460, Loss: 0.0803002825\n",
      "Epoch 4470, Loss: 0.0796425888\n",
      "Epoch 4480, Loss: 0.0805574150\n",
      "Epoch 4490, Loss: 0.0786379306\n",
      "Epoch 4500, Loss: 0.0778579809\n",
      "Epoch 4510, Loss: 0.0782940361\n",
      "Epoch 4520, Loss: 0.0792488319\n",
      "Epoch 4530, Loss: 0.0783819176\n",
      "Epoch 4540, Loss: 0.0799401706\n",
      "Epoch 4550, Loss: 0.0771538182\n",
      "Epoch 4560, Loss: 0.0791859241\n",
      "Epoch 4570, Loss: 0.0773797593\n",
      "Epoch 4580, Loss: 0.0798589936\n",
      "Epoch 4590, Loss: 0.0797191668\n",
      "Epoch 4600, Loss: 0.0784947285\n",
      "Epoch 4610, Loss: 0.0765692556\n",
      "Epoch 4620, Loss: 0.0782923371\n",
      "Epoch 4630, Loss: 0.0813938726\n",
      "Epoch 4640, Loss: 0.0767598540\n",
      "Epoch 4650, Loss: 0.0786479877\n",
      "Epoch 4660, Loss: 0.0786202207\n",
      "Epoch 4670, Loss: 0.0783783697\n",
      "Epoch 4680, Loss: 0.0802264497\n",
      "Epoch 4690, Loss: 0.0779325591\n",
      "Epoch 4700, Loss: 0.0792579891\n",
      "Epoch 4710, Loss: 0.0778051943\n",
      "Epoch 4720, Loss: 0.0790717044\n",
      "Epoch 4730, Loss: 0.0796060571\n",
      "Epoch 4740, Loss: 0.0770163073\n",
      "Epoch 4750, Loss: 0.0804929312\n",
      "Epoch 4760, Loss: 0.0774954484\n",
      "Epoch 4770, Loss: 0.0774711183\n",
      "Epoch 4780, Loss: 0.0772987832\n",
      "Epoch 4790, Loss: 0.0784445110\n",
      "Epoch 4800, Loss: 0.0786415792\n",
      "Epoch 4810, Loss: 0.0786704627\n",
      "Epoch 4820, Loss: 0.0788479561\n",
      "Epoch 4830, Loss: 0.0765256688\n",
      "Epoch 4840, Loss: 0.0792544656\n",
      "Epoch 4850, Loss: 0.0781871842\n",
      "Epoch 4860, Loss: 0.0775914167\n",
      "Epoch 4870, Loss: 0.0790561740\n",
      "Epoch 4880, Loss: 0.0809751117\n",
      "Epoch 4890, Loss: 0.0790504208\n",
      "Epoch 4900, Loss: 0.0772859821\n",
      "Epoch 4910, Loss: 0.0771607232\n",
      "Epoch 4920, Loss: 0.0787939344\n",
      "Epoch 4930, Loss: 0.0775370248\n",
      "Epoch 4940, Loss: 0.0792899115\n",
      "Epoch 4950, Loss: 0.0789872608\n",
      "Epoch 4960, Loss: 0.0767746768\n",
      "Epoch 4970, Loss: 0.0775903799\n",
      "Epoch 4980, Loss: 0.0783091736\n",
      "Epoch 4990, Loss: 0.0786704780\n",
      "Epoch 5000, Loss: 0.0785250354\n",
      "Epoch 5010, Loss: 0.0789578353\n",
      "Epoch 5020, Loss: 0.0769788738\n",
      "Epoch 5030, Loss: 0.0798161346\n",
      "Epoch 5040, Loss: 0.0791300049\n",
      "Epoch 5050, Loss: 0.0805918975\n",
      "Epoch 5060, Loss: 0.0768854555\n",
      "Epoch 5070, Loss: 0.0763357506\n",
      "Epoch 5080, Loss: 0.0799806476\n",
      "Epoch 5090, Loss: 0.0770963641\n",
      "Epoch 5100, Loss: 0.0786065985\n",
      "Epoch 5110, Loss: 0.0784303497\n",
      "Epoch 5120, Loss: 0.0798212492\n",
      "Epoch 5130, Loss: 0.0808339615\n",
      "Epoch 5140, Loss: 0.0775727239\n",
      "Epoch 5150, Loss: 0.0770063286\n",
      "Epoch 5160, Loss: 0.0775845270\n",
      "Epoch 5170, Loss: 0.0780592850\n",
      "Epoch 5180, Loss: 0.0795302175\n",
      "Epoch 5190, Loss: 0.0806135654\n",
      "Epoch 5200, Loss: 0.0774866001\n",
      "Epoch 5210, Loss: 0.0799507426\n",
      "Epoch 5220, Loss: 0.0791955392\n",
      "Epoch 5230, Loss: 0.0820242526\n",
      "Epoch 5240, Loss: 0.0771773371\n",
      "Epoch 5250, Loss: 0.0792874064\n",
      "Epoch 5260, Loss: 0.0782444328\n",
      "Epoch 5270, Loss: 0.0773801108\n",
      "Epoch 5280, Loss: 0.0796141714\n",
      "Epoch 5290, Loss: 0.0771073394\n",
      "Epoch 5300, Loss: 0.0810776460\n",
      "Epoch 5310, Loss: 0.0774758545\n",
      "Epoch 5320, Loss: 0.0768124682\n",
      "Epoch 5330, Loss: 0.0771206351\n",
      "Epoch 5340, Loss: 0.0778065015\n",
      "Epoch 5350, Loss: 0.0799310550\n",
      "Epoch 5360, Loss: 0.0768986611\n",
      "Epoch 5370, Loss: 0.0810460201\n",
      "Epoch 5380, Loss: 0.0781083280\n",
      "Epoch 5390, Loss: 0.0793414205\n",
      "Epoch 5400, Loss: 0.0771596749\n",
      "Epoch 5410, Loss: 0.0785955566\n",
      "Epoch 5420, Loss: 0.0786672398\n",
      "Epoch 5430, Loss: 0.0791120996\n",
      "Epoch 5440, Loss: 0.0783851135\n",
      "Epoch 5450, Loss: 0.0780784903\n",
      "Epoch 5460, Loss: 0.0787977067\n",
      "Epoch 5470, Loss: 0.0807875970\n",
      "Epoch 5480, Loss: 0.0780507976\n",
      "Epoch 5490, Loss: 0.0791592199\n",
      "Epoch 5500, Loss: 0.0825913019\n",
      "Epoch 5510, Loss: 0.0800678593\n",
      "Epoch 5520, Loss: 0.0787810981\n",
      "Epoch 5530, Loss: 0.0771330969\n",
      "Epoch 5540, Loss: 0.0793570057\n",
      "Epoch 5550, Loss: 0.0776137242\n",
      "Epoch 5560, Loss: 0.0769082469\n",
      "Epoch 5570, Loss: 0.0800288866\n",
      "Epoch 5580, Loss: 0.0783674940\n",
      "Epoch 5590, Loss: 0.0779548699\n",
      "Epoch 5600, Loss: 0.0795716938\n",
      "Epoch 5610, Loss: 0.0779143991\n",
      "Epoch 5620, Loss: 0.0770692188\n",
      "Epoch 5630, Loss: 0.0770554866\n",
      "Epoch 5640, Loss: 0.0778904486\n",
      "Epoch 5650, Loss: 0.0781467277\n",
      "Epoch 5660, Loss: 0.0772412306\n",
      "Epoch 5670, Loss: 0.0778518516\n",
      "Epoch 5680, Loss: 0.0780995050\n",
      "Epoch 5690, Loss: 0.0776011078\n",
      "Epoch 5700, Loss: 0.0774375033\n",
      "Epoch 5710, Loss: 0.0774967010\n",
      "Epoch 5720, Loss: 0.0775084707\n",
      "Epoch 5730, Loss: 0.0796448739\n",
      "Epoch 5740, Loss: 0.0768428989\n",
      "Epoch 5750, Loss: 0.0803032005\n",
      "Epoch 5760, Loss: 0.0805259480\n",
      "Epoch 5770, Loss: 0.0778303467\n",
      "Epoch 5780, Loss: 0.0780658464\n",
      "Epoch 5790, Loss: 0.0795524088\n",
      "Epoch 5800, Loss: 0.0785151529\n",
      "Epoch 5810, Loss: 0.0768265998\n",
      "Epoch 5820, Loss: 0.0776950996\n",
      "Epoch 5830, Loss: 0.0766920845\n",
      "Epoch 5840, Loss: 0.0772532614\n",
      "Epoch 5850, Loss: 0.0795186986\n",
      "Epoch 5860, Loss: 0.0802437590\n",
      "Epoch 5870, Loss: 0.0785525295\n",
      "Epoch 5880, Loss: 0.0781895323\n",
      "Epoch 5890, Loss: 0.0776081783\n",
      "Epoch 5900, Loss: 0.0812376952\n",
      "Epoch 5910, Loss: 0.0791356939\n",
      "Epoch 5920, Loss: 0.0767587943\n",
      "Epoch 5930, Loss: 0.0766357155\n",
      "Epoch 5940, Loss: 0.0780256726\n",
      "Epoch 5950, Loss: 0.0765326498\n",
      "Epoch 5960, Loss: 0.0774243870\n",
      "Epoch 5970, Loss: 0.0776300667\n",
      "Epoch 5980, Loss: 0.0769915457\n",
      "Epoch 5990, Loss: 0.0788391848\n",
      "Epoch 6000, Loss: 0.0785055438\n",
      "Epoch 6010, Loss: 0.0798261607\n",
      "Epoch 6020, Loss: 0.0784155345\n",
      "Epoch 6030, Loss: 0.0772666904\n",
      "Epoch 6040, Loss: 0.0773609240\n",
      "Epoch 6050, Loss: 0.0829727943\n",
      "Epoch 6060, Loss: 0.0798378652\n",
      "Epoch 6070, Loss: 0.0784294212\n",
      "Epoch 6080, Loss: 0.0763892230\n",
      "Epoch 6090, Loss: 0.0797433282\n",
      "Epoch 6100, Loss: 0.0786847782\n",
      "Epoch 6110, Loss: 0.0794426787\n",
      "Epoch 6120, Loss: 0.0791751292\n",
      "Epoch 6130, Loss: 0.0795024666\n",
      "Epoch 6140, Loss: 0.0770750892\n",
      "Epoch 6150, Loss: 0.0782547866\n",
      "Epoch 6160, Loss: 0.0804684439\n",
      "Epoch 6170, Loss: 0.0810411513\n",
      "Epoch 6180, Loss: 0.0788462952\n",
      "Epoch 6190, Loss: 0.0797684310\n",
      "Epoch 6200, Loss: 0.0778555237\n",
      "Epoch 6210, Loss: 0.0773006126\n",
      "Epoch 6220, Loss: 0.0791646865\n",
      "Epoch 6230, Loss: 0.0798903218\n",
      "Epoch 6240, Loss: 0.0777086490\n",
      "Epoch 6250, Loss: 0.0775585006\n",
      "Epoch 6260, Loss: 0.0769136336\n",
      "Epoch 6270, Loss: 0.0785052434\n",
      "Epoch 6280, Loss: 0.0765169632\n",
      "Epoch 6290, Loss: 0.0757506329\n",
      "Epoch 6300, Loss: 0.0809431943\n",
      "Epoch 6310, Loss: 0.0789257067\n",
      "Epoch 6320, Loss: 0.0765174824\n",
      "Epoch 6330, Loss: 0.0783762994\n",
      "Epoch 6340, Loss: 0.0763079853\n",
      "Epoch 6350, Loss: 0.0792309681\n",
      "Epoch 6360, Loss: 0.0780252884\n",
      "Epoch 6370, Loss: 0.0788807628\n",
      "Epoch 6380, Loss: 0.0782569543\n",
      "Epoch 6390, Loss: 0.0778631306\n",
      "Epoch 6400, Loss: 0.0765494642\n",
      "Epoch 6410, Loss: 0.0795282012\n",
      "Epoch 6420, Loss: 0.0755428705\n",
      "Epoch 6430, Loss: 0.0771289657\n",
      "Epoch 6440, Loss: 0.0784635824\n",
      "Epoch 6450, Loss: 0.0790719843\n",
      "Epoch 6460, Loss: 0.0759529281\n",
      "Epoch 6470, Loss: 0.0779624347\n",
      "Epoch 6480, Loss: 0.0779823047\n",
      "Epoch 6490, Loss: 0.0774248813\n",
      "Epoch 6500, Loss: 0.0782411250\n",
      "Epoch 6510, Loss: 0.0764013137\n",
      "Epoch 6520, Loss: 0.0775471802\n",
      "Epoch 6530, Loss: 0.0771045746\n",
      "Epoch 6540, Loss: 0.0808987953\n",
      "Epoch 6550, Loss: 0.0798719326\n",
      "Epoch 6560, Loss: 0.0795682055\n",
      "Epoch 6570, Loss: 0.0777340655\n",
      "Epoch 6580, Loss: 0.0760483359\n",
      "Epoch 6590, Loss: 0.0762664761\n",
      "Epoch 6600, Loss: 0.0772620386\n",
      "Epoch 6610, Loss: 0.0787924549\n",
      "Epoch 6620, Loss: 0.0780401193\n",
      "Epoch 6630, Loss: 0.0784077018\n",
      "Epoch 6640, Loss: 0.0762540531\n",
      "Epoch 6650, Loss: 0.0787449487\n",
      "Epoch 6660, Loss: 0.0781471011\n",
      "Epoch 6670, Loss: 0.0813543543\n",
      "Epoch 6680, Loss: 0.0797522107\n",
      "Epoch 6690, Loss: 0.0772259122\n",
      "Epoch 6700, Loss: 0.0765507136\n",
      "Epoch 6710, Loss: 0.0754921234\n",
      "Epoch 6720, Loss: 0.0772263944\n",
      "Epoch 6730, Loss: 0.0776340073\n",
      "Epoch 6740, Loss: 0.0792601094\n",
      "Epoch 6750, Loss: 0.0783201456\n",
      "Epoch 6760, Loss: 0.0770787205\n",
      "Epoch 6770, Loss: 0.0758282530\n",
      "Epoch 6780, Loss: 0.0772160143\n",
      "Epoch 6790, Loss: 0.0775146581\n",
      "Epoch 6800, Loss: 0.0781915580\n",
      "Epoch 6810, Loss: 0.0781767422\n",
      "Epoch 6820, Loss: 0.0763084613\n",
      "Epoch 6830, Loss: 0.0782885304\n",
      "Epoch 6840, Loss: 0.0795390062\n",
      "Epoch 6850, Loss: 0.0767784876\n",
      "Epoch 6860, Loss: 0.0775273137\n",
      "Epoch 6870, Loss: 0.0766959806\n",
      "Epoch 6880, Loss: 0.0764367822\n",
      "Epoch 6890, Loss: 0.0763629133\n",
      "Epoch 6900, Loss: 0.0784253509\n",
      "Epoch 6910, Loss: 0.0771883163\n",
      "Epoch 6920, Loss: 0.0772911763\n",
      "Epoch 6930, Loss: 0.0777496751\n",
      "Epoch 6940, Loss: 0.0806419724\n",
      "Epoch 6950, Loss: 0.0797351557\n",
      "Epoch 6960, Loss: 0.0781854519\n",
      "Epoch 6970, Loss: 0.0799117386\n",
      "Epoch 6980, Loss: 0.0786720861\n",
      "Epoch 6990, Loss: 0.0761510147\n",
      "Epoch 7000, Loss: 0.0772843768\n",
      "Epoch 7010, Loss: 0.0785213019\n",
      "Epoch 7020, Loss: 0.0774540104\n",
      "Epoch 7030, Loss: 0.0776456808\n",
      "Epoch 7040, Loss: 0.0759906331\n",
      "Epoch 7050, Loss: 0.0770488206\n",
      "Epoch 7060, Loss: 0.0768568602\n",
      "Epoch 7070, Loss: 0.0774643537\n",
      "Epoch 7080, Loss: 0.0815415823\n",
      "Epoch 7090, Loss: 0.0779724757\n",
      "Epoch 7100, Loss: 0.0761456913\n",
      "Epoch 7110, Loss: 0.0783595981\n",
      "Epoch 7120, Loss: 0.0779008110\n",
      "Epoch 7130, Loss: 0.0767144151\n",
      "Epoch 7140, Loss: 0.0772017379\n",
      "Epoch 7150, Loss: 0.0759543336\n",
      "Epoch 7160, Loss: 0.0792399845\n",
      "Epoch 7170, Loss: 0.0765589157\n",
      "Epoch 7180, Loss: 0.0767786287\n",
      "Epoch 7190, Loss: 0.0772460943\n",
      "Epoch 7200, Loss: 0.0767284732\n",
      "Epoch 7210, Loss: 0.0758861461\n",
      "Epoch 7220, Loss: 0.0759713826\n",
      "Epoch 7230, Loss: 0.0756112762\n",
      "Epoch 7240, Loss: 0.0796861650\n",
      "Epoch 7250, Loss: 0.0805163276\n",
      "Epoch 7260, Loss: 0.0782936713\n",
      "Epoch 7270, Loss: 0.0790908587\n",
      "Epoch 7280, Loss: 0.0822122581\n",
      "Epoch 7290, Loss: 0.0780332682\n",
      "Epoch 7300, Loss: 0.0782861289\n",
      "Epoch 7310, Loss: 0.0778882143\n",
      "Epoch 7320, Loss: 0.0765258351\n",
      "Epoch 7330, Loss: 0.0767336494\n",
      "Epoch 7340, Loss: 0.0795856644\n",
      "Epoch 7350, Loss: 0.0787621936\n",
      "Epoch 7360, Loss: 0.0793057357\n",
      "Epoch 7370, Loss: 0.0783308430\n",
      "Epoch 7380, Loss: 0.0782228196\n",
      "Epoch 7390, Loss: 0.0755590543\n",
      "Epoch 7400, Loss: 0.0757461523\n",
      "Epoch 7410, Loss: 0.0782088502\n",
      "Epoch 7420, Loss: 0.0752070981\n",
      "Epoch 7430, Loss: 0.0785334413\n",
      "Epoch 7440, Loss: 0.0790494499\n",
      "Epoch 7450, Loss: 0.0763296470\n",
      "Epoch 7460, Loss: 0.0785124634\n",
      "Epoch 7470, Loss: 0.0781035911\n",
      "Epoch 7480, Loss: 0.0771210892\n",
      "Epoch 7490, Loss: 0.0774995779\n",
      "Epoch 7500, Loss: 0.0779522896\n",
      "Epoch 7510, Loss: 0.0780607200\n",
      "Epoch 7520, Loss: 0.0775556950\n",
      "Epoch 7530, Loss: 0.0797065963\n",
      "Epoch 7540, Loss: 0.0777443472\n",
      "Epoch 7550, Loss: 0.0773209780\n",
      "Epoch 7560, Loss: 0.0757200915\n",
      "Epoch 7570, Loss: 0.0779393739\n",
      "Epoch 7580, Loss: 0.0761703898\n",
      "Epoch 7590, Loss: 0.0763494703\n",
      "Epoch 7600, Loss: 0.0778502007\n",
      "Epoch 7610, Loss: 0.0796517800\n",
      "Epoch 7620, Loss: 0.0780317523\n",
      "Epoch 7630, Loss: 0.0791946703\n",
      "Epoch 7640, Loss: 0.0765828216\n",
      "Epoch 7650, Loss: 0.0792010865\n",
      "Epoch 7660, Loss: 0.0761867428\n",
      "Epoch 7670, Loss: 0.0764088865\n",
      "Epoch 7680, Loss: 0.0808360007\n",
      "Epoch 7690, Loss: 0.0771119903\n",
      "Epoch 7700, Loss: 0.0756457064\n",
      "Epoch 7710, Loss: 0.0776418318\n",
      "Epoch 7720, Loss: 0.0771322717\n",
      "Epoch 7730, Loss: 0.0759035233\n",
      "Epoch 7740, Loss: 0.0765872306\n",
      "Epoch 7750, Loss: 0.0770618403\n",
      "Epoch 7760, Loss: 0.0774719582\n",
      "Epoch 7770, Loss: 0.0787021588\n",
      "Epoch 7780, Loss: 0.0766384614\n",
      "Epoch 7790, Loss: 0.0755556811\n",
      "Epoch 7800, Loss: 0.0768488210\n",
      "Epoch 7810, Loss: 0.0762193255\n",
      "Epoch 7820, Loss: 0.0755889217\n",
      "Epoch 7830, Loss: 0.0755512024\n",
      "Epoch 7840, Loss: 0.0781258227\n",
      "Epoch 7850, Loss: 0.0778911762\n",
      "Epoch 7860, Loss: 0.0755537464\n",
      "Epoch 7870, Loss: 0.0793857810\n",
      "Epoch 7880, Loss: 0.0780388926\n",
      "Epoch 7890, Loss: 0.0779252292\n",
      "Epoch 7900, Loss: 0.0771174207\n",
      "Epoch 7910, Loss: 0.0758352180\n",
      "Epoch 7920, Loss: 0.0776121072\n",
      "Epoch 7930, Loss: 0.0779033095\n",
      "Epoch 7940, Loss: 0.0773315345\n",
      "Epoch 7950, Loss: 0.0769544205\n",
      "Epoch 7960, Loss: 0.0761925571\n",
      "Epoch 7970, Loss: 0.0782366978\n",
      "Epoch 7980, Loss: 0.0752161033\n",
      "Epoch 7990, Loss: 0.0767665724\n",
      "Epoch 8000, Loss: 0.0757977082\n",
      "Epoch 8010, Loss: 0.0795542225\n",
      "Epoch 8020, Loss: 0.0768308651\n",
      "Epoch 8030, Loss: 0.0775082481\n",
      "Epoch 8040, Loss: 0.0760138100\n",
      "Epoch 8050, Loss: 0.0773746520\n",
      "Epoch 8060, Loss: 0.0781181429\n",
      "Epoch 8070, Loss: 0.0768844269\n",
      "Epoch 8080, Loss: 0.0780387864\n",
      "Epoch 8090, Loss: 0.0770881918\n",
      "Epoch 8100, Loss: 0.0769918646\n",
      "Epoch 8110, Loss: 0.0760043909\n",
      "Epoch 8120, Loss: 0.0761578877\n",
      "Epoch 8130, Loss: 0.0751944166\n",
      "Epoch 8140, Loss: 0.0785511628\n",
      "Epoch 8150, Loss: 0.0780652238\n",
      "Epoch 8160, Loss: 0.0782207507\n",
      "Epoch 8170, Loss: 0.0753364773\n",
      "Epoch 8180, Loss: 0.0759132697\n",
      "Epoch 8190, Loss: 0.0798148774\n",
      "Epoch 8200, Loss: 0.0780908071\n",
      "Epoch 8210, Loss: 0.0762290217\n",
      "Epoch 8220, Loss: 0.0781528583\n",
      "Epoch 8230, Loss: 0.0792115143\n",
      "Epoch 8240, Loss: 0.0769932531\n",
      "Epoch 8250, Loss: 0.0778588895\n",
      "Epoch 8260, Loss: 0.0758194683\n",
      "Epoch 8270, Loss: 0.0752330922\n",
      "Epoch 8280, Loss: 0.0787563729\n",
      "Epoch 8290, Loss: 0.0764560397\n",
      "Epoch 8300, Loss: 0.0771511067\n",
      "Epoch 8310, Loss: 0.0797226422\n",
      "Epoch 8320, Loss: 0.0791774368\n",
      "Epoch 8330, Loss: 0.0762852602\n",
      "Epoch 8340, Loss: 0.0779690006\n",
      "Epoch 8350, Loss: 0.0771432394\n",
      "Epoch 8360, Loss: 0.0795621617\n",
      "Epoch 8370, Loss: 0.0778827482\n",
      "Epoch 8380, Loss: 0.0774594878\n",
      "Epoch 8390, Loss: 0.0767565082\n",
      "Epoch 8400, Loss: 0.0773517053\n",
      "Epoch 8410, Loss: 0.0752332319\n",
      "Epoch 8420, Loss: 0.0765651083\n",
      "Epoch 8430, Loss: 0.0776838207\n",
      "Epoch 8440, Loss: 0.0781495024\n",
      "Epoch 8450, Loss: 0.0769557697\n",
      "Epoch 8460, Loss: 0.0767717518\n",
      "Epoch 8470, Loss: 0.0757784852\n",
      "Epoch 8480, Loss: 0.0811579835\n",
      "Epoch 8490, Loss: 0.0757384971\n",
      "Epoch 8500, Loss: 0.0779355370\n",
      "Epoch 8510, Loss: 0.0754652405\n",
      "Epoch 8520, Loss: 0.0774329735\n",
      "Epoch 8530, Loss: 0.0744317499\n",
      "Epoch 8540, Loss: 0.0762532946\n",
      "Epoch 8550, Loss: 0.0770252717\n",
      "Epoch 8560, Loss: 0.0762982193\n",
      "Epoch 8570, Loss: 0.0775372435\n",
      "Epoch 8580, Loss: 0.0756173304\n",
      "Epoch 8590, Loss: 0.0774039063\n",
      "Epoch 8600, Loss: 0.0786682414\n",
      "Epoch 8610, Loss: 0.0783458675\n",
      "Epoch 8620, Loss: 0.0766960008\n",
      "Epoch 8630, Loss: 0.0768828976\n",
      "Epoch 8640, Loss: 0.0769822809\n",
      "Epoch 8650, Loss: 0.0795671591\n",
      "Epoch 8660, Loss: 0.0785627544\n",
      "Epoch 8670, Loss: 0.0755017404\n",
      "Epoch 8680, Loss: 0.0785771875\n",
      "Epoch 8690, Loss: 0.0790479764\n",
      "Epoch 8700, Loss: 0.0775132367\n",
      "Epoch 8710, Loss: 0.0756378758\n",
      "Epoch 8720, Loss: 0.0770103230\n",
      "Epoch 8730, Loss: 0.0765183401\n",
      "Epoch 8740, Loss: 0.0816157649\n",
      "Epoch 8750, Loss: 0.0767593250\n",
      "Epoch 8760, Loss: 0.0771526095\n",
      "Epoch 8770, Loss: 0.0748847272\n",
      "Epoch 8780, Loss: 0.0750094500\n",
      "Epoch 8790, Loss: 0.0785905716\n",
      "Epoch 8800, Loss: 0.0758802359\n",
      "Epoch 8810, Loss: 0.0769274178\n",
      "Epoch 8820, Loss: 0.0764283098\n",
      "Epoch 8830, Loss: 0.0754555750\n",
      "Epoch 8840, Loss: 0.0759505944\n",
      "Epoch 8850, Loss: 0.0753332499\n",
      "Epoch 8860, Loss: 0.0755237249\n",
      "Epoch 8870, Loss: 0.0747250848\n",
      "Epoch 8880, Loss: 0.0762920810\n",
      "Epoch 8890, Loss: 0.0777946408\n",
      "Epoch 8900, Loss: 0.0757773794\n",
      "Epoch 8910, Loss: 0.0752080490\n",
      "Epoch 8920, Loss: 0.0796848997\n",
      "Epoch 8930, Loss: 0.0768382798\n",
      "Epoch 8940, Loss: 0.0780968736\n",
      "Epoch 8950, Loss: 0.0746042364\n",
      "Epoch 8960, Loss: 0.0787791300\n",
      "Epoch 8970, Loss: 0.0806759587\n",
      "Epoch 8980, Loss: 0.0786946195\n",
      "Epoch 8990, Loss: 0.0752323206\n",
      "Epoch 9000, Loss: 0.0789307360\n",
      "Epoch 9010, Loss: 0.0774102989\n",
      "Epoch 9020, Loss: 0.0774859126\n",
      "Epoch 9030, Loss: 0.0756722133\n",
      "Epoch 9040, Loss: 0.0748254023\n",
      "Epoch 9050, Loss: 0.0755849822\n",
      "Epoch 9060, Loss: 0.0785899845\n",
      "Epoch 9070, Loss: 0.0777062720\n",
      "Epoch 9080, Loss: 0.0763875289\n",
      "Epoch 9090, Loss: 0.0767592028\n",
      "Epoch 9100, Loss: 0.0774860105\n",
      "Epoch 9110, Loss: 0.0756038281\n",
      "Epoch 9120, Loss: 0.0757740971\n",
      "Epoch 9130, Loss: 0.0764495766\n",
      "Epoch 9140, Loss: 0.0767772159\n",
      "Epoch 9150, Loss: 0.0761746641\n",
      "Epoch 9160, Loss: 0.0769874284\n",
      "Epoch 9170, Loss: 0.0762621963\n",
      "Epoch 9180, Loss: 0.0761737309\n",
      "Epoch 9190, Loss: 0.0778115679\n",
      "Epoch 9200, Loss: 0.0767811841\n",
      "Epoch 9210, Loss: 0.0791795079\n",
      "Epoch 9220, Loss: 0.0751920387\n",
      "Epoch 9230, Loss: 0.0774079250\n",
      "Epoch 9240, Loss: 0.0747289938\n",
      "Epoch 9250, Loss: 0.0784346309\n",
      "Epoch 9260, Loss: 0.0765500471\n",
      "Epoch 9270, Loss: 0.0766043168\n",
      "Epoch 9280, Loss: 0.0755537920\n",
      "Epoch 9290, Loss: 0.0750730349\n",
      "Epoch 9300, Loss: 0.0779443556\n",
      "Epoch 9310, Loss: 0.0779166254\n",
      "Epoch 9320, Loss: 0.0785148649\n",
      "Epoch 9330, Loss: 0.0790786132\n",
      "Epoch 9340, Loss: 0.0748915824\n",
      "Epoch 9350, Loss: 0.0781683506\n",
      "Epoch 9360, Loss: 0.0775669895\n",
      "Epoch 9370, Loss: 0.0762315899\n",
      "Epoch 9380, Loss: 0.0764277303\n",
      "Epoch 9390, Loss: 0.0790723628\n",
      "Epoch 9400, Loss: 0.0746820226\n",
      "Epoch 9410, Loss: 0.0764914452\n",
      "Epoch 9420, Loss: 0.0750328112\n",
      "Epoch 9430, Loss: 0.0767103733\n",
      "Epoch 9440, Loss: 0.0795902249\n",
      "Epoch 9450, Loss: 0.0751073155\n",
      "Epoch 9460, Loss: 0.0756619956\n",
      "Epoch 9470, Loss: 0.0752569108\n",
      "Epoch 9480, Loss: 0.0737983416\n",
      "Epoch 9490, Loss: 0.0772130425\n",
      "Epoch 9500, Loss: 0.0776855421\n",
      "Epoch 9510, Loss: 0.0750583246\n",
      "Epoch 9520, Loss: 0.0744331775\n",
      "Epoch 9530, Loss: 0.0758178531\n",
      "Epoch 9540, Loss: 0.0747792284\n",
      "Epoch 9550, Loss: 0.0746780942\n",
      "Epoch 9560, Loss: 0.0765946864\n",
      "Epoch 9570, Loss: 0.0786866553\n",
      "Epoch 9580, Loss: 0.0757135583\n",
      "Epoch 9590, Loss: 0.0776291870\n",
      "Epoch 9600, Loss: 0.0771493061\n",
      "Epoch 9610, Loss: 0.0766480500\n",
      "Epoch 9620, Loss: 0.0776350853\n",
      "Epoch 9630, Loss: 0.0768627737\n",
      "Epoch 9640, Loss: 0.0776686373\n",
      "Epoch 9650, Loss: 0.0768186678\n",
      "Epoch 9660, Loss: 0.0756630120\n",
      "Epoch 9670, Loss: 0.0769951293\n",
      "Epoch 9680, Loss: 0.0759106034\n",
      "Epoch 9690, Loss: 0.0773318057\n",
      "Epoch 9700, Loss: 0.0786486456\n",
      "Epoch 9710, Loss: 0.0756456247\n",
      "Epoch 9720, Loss: 0.0765813133\n",
      "Epoch 9730, Loss: 0.0747033566\n",
      "Epoch 9740, Loss: 0.0754191686\n",
      "Epoch 9750, Loss: 0.0788895687\n",
      "Epoch 9760, Loss: 0.0756857654\n",
      "Epoch 9770, Loss: 0.0809904741\n",
      "Epoch 9780, Loss: 0.0743982943\n",
      "Epoch 9790, Loss: 0.0750903326\n",
      "Epoch 9800, Loss: 0.0752043031\n",
      "Epoch 9810, Loss: 0.0743912055\n",
      "Epoch 9820, Loss: 0.0749067992\n",
      "Epoch 9830, Loss: 0.0755185801\n",
      "Epoch 9840, Loss: 0.0758675875\n",
      "Epoch 9850, Loss: 0.0757345290\n",
      "Epoch 9860, Loss: 0.0742423168\n",
      "Epoch 9870, Loss: 0.0780299121\n",
      "Epoch 9880, Loss: 0.0762395926\n",
      "Epoch 9890, Loss: 0.0743967313\n",
      "Epoch 9900, Loss: 0.0765182604\n",
      "Epoch 9910, Loss: 0.0774111409\n",
      "Epoch 9920, Loss: 0.0744256958\n",
      "Epoch 9930, Loss: 0.0749358143\n",
      "Epoch 9940, Loss: 0.0786348428\n",
      "Epoch 9950, Loss: 0.0756354745\n",
      "Epoch 9960, Loss: 0.0782233773\n",
      "Epoch 9970, Loss: 0.0769796020\n",
      "Epoch 9980, Loss: 0.0758418766\n",
      "Epoch 9990, Loss: 0.0753476727\n",
      "Epoch 10000, Loss: 0.0749566956\n",
      "Epoch 10010, Loss: 0.0756138211\n",
      "Epoch 10020, Loss: 0.0764793931\n",
      "Epoch 10030, Loss: 0.0762598340\n",
      "Epoch 10040, Loss: 0.0780461237\n",
      "Epoch 10050, Loss: 0.0745078601\n",
      "Epoch 10060, Loss: 0.0756434178\n",
      "Epoch 10070, Loss: 0.0757682060\n",
      "Epoch 10080, Loss: 0.0753805107\n",
      "Epoch 10090, Loss: 0.0753319241\n",
      "Epoch 10100, Loss: 0.0747277604\n",
      "Epoch 10110, Loss: 0.0742509677\n",
      "Epoch 10120, Loss: 0.0748727758\n",
      "Epoch 10130, Loss: 0.0744535325\n",
      "Epoch 10140, Loss: 0.0752161880\n",
      "Epoch 10150, Loss: 0.0758819859\n",
      "Epoch 10160, Loss: 0.0755415489\n",
      "Epoch 10170, Loss: 0.0783520114\n",
      "Epoch 10180, Loss: 0.0769273847\n",
      "Epoch 10190, Loss: 0.0760218669\n",
      "Epoch 10200, Loss: 0.0754378784\n",
      "Epoch 10210, Loss: 0.0765297879\n",
      "Epoch 10220, Loss: 0.0741807427\n",
      "Epoch 10230, Loss: 0.0771929212\n",
      "Epoch 10240, Loss: 0.0781229971\n",
      "Epoch 10250, Loss: 0.0760176271\n",
      "Epoch 10260, Loss: 0.0763173436\n",
      "Epoch 10270, Loss: 0.0763324852\n",
      "Epoch 10280, Loss: 0.0743183243\n",
      "Epoch 10290, Loss: 0.0761097535\n",
      "Epoch 10300, Loss: 0.0781975846\n",
      "Epoch 10310, Loss: 0.0757370503\n",
      "Epoch 10320, Loss: 0.0759109610\n",
      "Epoch 10330, Loss: 0.0743739189\n",
      "Epoch 10340, Loss: 0.0750996062\n",
      "Epoch 10350, Loss: 0.0736950908\n",
      "Epoch 10360, Loss: 0.0757389487\n",
      "Epoch 10370, Loss: 0.0779523280\n",
      "Epoch 10380, Loss: 0.0744731554\n",
      "Epoch 10390, Loss: 0.0757238263\n",
      "Epoch 10400, Loss: 0.0748615159\n",
      "Epoch 10410, Loss: 0.0771391448\n",
      "Epoch 10420, Loss: 0.0737846697\n",
      "Epoch 10430, Loss: 0.0774858285\n",
      "Epoch 10440, Loss: 0.0738869026\n",
      "Epoch 10450, Loss: 0.0755873513\n",
      "Epoch 10460, Loss: 0.0736960612\n",
      "Epoch 10470, Loss: 0.0775881794\n",
      "Epoch 10480, Loss: 0.0766006760\n",
      "Epoch 10490, Loss: 0.0746738556\n",
      "Epoch 10500, Loss: 0.0770120288\n",
      "Epoch 10510, Loss: 0.0749429362\n",
      "Epoch 10520, Loss: 0.0757755224\n",
      "Epoch 10530, Loss: 0.0760620925\n",
      "Epoch 10540, Loss: 0.0778561205\n",
      "Epoch 10550, Loss: 0.0776526840\n",
      "Epoch 10560, Loss: 0.0756543973\n",
      "Epoch 10570, Loss: 0.0761176917\n",
      "Epoch 10580, Loss: 0.0734862123\n",
      "Epoch 10590, Loss: 0.0761106306\n",
      "Epoch 10600, Loss: 0.0764400491\n",
      "Epoch 10610, Loss: 0.0748546606\n",
      "Epoch 10620, Loss: 0.0734890200\n",
      "Epoch 10630, Loss: 0.0736792643\n",
      "Epoch 10640, Loss: 0.0743652670\n",
      "Epoch 10650, Loss: 0.0763022006\n",
      "Epoch 10660, Loss: 0.0744389878\n",
      "Epoch 10670, Loss: 0.0759792838\n",
      "Epoch 10680, Loss: 0.0779042476\n",
      "Epoch 10690, Loss: 0.0775184254\n",
      "Epoch 10700, Loss: 0.0744445159\n",
      "Epoch 10710, Loss: 0.0760035965\n",
      "Epoch 10720, Loss: 0.0751230912\n",
      "Epoch 10730, Loss: 0.0755693953\n",
      "Epoch 10740, Loss: 0.0743602324\n",
      "Epoch 10750, Loss: 0.0756267226\n",
      "Epoch 10760, Loss: 0.0753091842\n",
      "Epoch 10770, Loss: 0.0756095193\n",
      "Epoch 10780, Loss: 0.0763627230\n",
      "Epoch 10790, Loss: 0.0755492438\n",
      "Epoch 10800, Loss: 0.0737842485\n",
      "Epoch 10810, Loss: 0.0763224126\n",
      "Epoch 10820, Loss: 0.0744807644\n",
      "Epoch 10830, Loss: 0.0742524266\n",
      "Epoch 10840, Loss: 0.0739492688\n",
      "Epoch 10850, Loss: 0.0781959081\n",
      "Epoch 10860, Loss: 0.0732285281\n",
      "Epoch 10870, Loss: 0.0740279719\n",
      "Epoch 10880, Loss: 0.0739970511\n",
      "Epoch 10890, Loss: 0.0748956253\n",
      "Epoch 10900, Loss: 0.0769131589\n",
      "Epoch 10910, Loss: 0.0737777142\n",
      "Epoch 10920, Loss: 0.0732681863\n",
      "Epoch 10930, Loss: 0.0729049869\n",
      "Epoch 10940, Loss: 0.0757609680\n",
      "Epoch 10950, Loss: 0.0775793312\n",
      "Epoch 10960, Loss: 0.0756817077\n",
      "Epoch 10970, Loss: 0.0752361888\n",
      "Epoch 10980, Loss: 0.0749076089\n",
      "Epoch 10990, Loss: 0.0778284062\n",
      "Epoch 11000, Loss: 0.0776006614\n",
      "Epoch 11010, Loss: 0.0770813261\n",
      "Epoch 11020, Loss: 0.0733403335\n",
      "Epoch 11030, Loss: 0.0749141125\n",
      "Epoch 11040, Loss: 0.0761442044\n",
      "Epoch 11050, Loss: 0.0738027667\n",
      "Epoch 11060, Loss: 0.0734391150\n",
      "Epoch 11070, Loss: 0.0729440892\n",
      "Epoch 11080, Loss: 0.0730560856\n",
      "Epoch 11090, Loss: 0.0745408885\n",
      "Epoch 11100, Loss: 0.0762047622\n",
      "Epoch 11110, Loss: 0.0752737783\n",
      "Epoch 11120, Loss: 0.0744621798\n",
      "Epoch 11130, Loss: 0.0750793630\n",
      "Epoch 11140, Loss: 0.0772795443\n",
      "Epoch 11150, Loss: 0.0740720174\n",
      "Epoch 11160, Loss: 0.0727948770\n",
      "Epoch 11170, Loss: 0.0759447061\n",
      "Epoch 11180, Loss: 0.0744967157\n",
      "Epoch 11190, Loss: 0.0748317498\n",
      "Epoch 11200, Loss: 0.0747501047\n",
      "Epoch 11210, Loss: 0.0760885018\n",
      "Epoch 11220, Loss: 0.0741874957\n",
      "Epoch 11230, Loss: 0.0740738285\n",
      "Epoch 11240, Loss: 0.0740639843\n",
      "Epoch 11250, Loss: 0.0731704849\n",
      "Epoch 11260, Loss: 0.0768185335\n",
      "Epoch 11270, Loss: 0.0733954608\n",
      "Epoch 11280, Loss: 0.0760652615\n",
      "Epoch 11290, Loss: 0.0746719192\n",
      "Epoch 11300, Loss: 0.0751002125\n",
      "Epoch 11310, Loss: 0.0742825648\n",
      "Epoch 11320, Loss: 0.0749324767\n",
      "Epoch 11330, Loss: 0.0727058230\n",
      "Epoch 11340, Loss: 0.0760041913\n",
      "Epoch 11350, Loss: 0.0736009096\n",
      "Epoch 11360, Loss: 0.0732982752\n",
      "Epoch 11370, Loss: 0.0755131009\n",
      "Epoch 11380, Loss: 0.0753975703\n",
      "Epoch 11390, Loss: 0.0744919027\n",
      "Epoch 11400, Loss: 0.0732365586\n",
      "Epoch 11410, Loss: 0.0774310370\n",
      "Epoch 11420, Loss: 0.0739709452\n",
      "Epoch 11430, Loss: 0.0729263650\n",
      "Epoch 11440, Loss: 0.0741341183\n",
      "Epoch 11450, Loss: 0.0749254206\n",
      "Epoch 11460, Loss: 0.0728633930\n",
      "Epoch 11470, Loss: 0.0750693889\n",
      "Epoch 11480, Loss: 0.0765260355\n",
      "Epoch 11490, Loss: 0.0740439202\n",
      "Epoch 11500, Loss: 0.0731497832\n",
      "Epoch 11510, Loss: 0.0740167320\n",
      "Epoch 11520, Loss: 0.0745768051\n",
      "Epoch 11530, Loss: 0.0767811210\n",
      "Epoch 11540, Loss: 0.0742555638\n",
      "Epoch 11550, Loss: 0.0735668498\n",
      "Epoch 11560, Loss: 0.0759497291\n",
      "Epoch 11570, Loss: 0.0744046374\n",
      "Epoch 11580, Loss: 0.0739765438\n",
      "Epoch 11590, Loss: 0.0747395079\n",
      "Epoch 11600, Loss: 0.0730314931\n",
      "Epoch 11610, Loss: 0.0734855439\n",
      "Epoch 11620, Loss: 0.0767277078\n",
      "Epoch 11630, Loss: 0.0780131994\n",
      "Epoch 11640, Loss: 0.0744414142\n",
      "Epoch 11650, Loss: 0.0730959183\n",
      "Epoch 11660, Loss: 0.0787090619\n",
      "Epoch 11670, Loss: 0.0757424682\n",
      "Epoch 11680, Loss: 0.0759075145\n",
      "Epoch 11690, Loss: 0.0748427197\n",
      "Epoch 11700, Loss: 0.0731306985\n",
      "Epoch 11710, Loss: 0.0724438778\n",
      "Epoch 11720, Loss: 0.0744202636\n",
      "Epoch 11730, Loss: 0.0748627469\n",
      "Epoch 11740, Loss: 0.0729689193\n",
      "Epoch 11750, Loss: 0.0723028286\n",
      "Epoch 11760, Loss: 0.0738209826\n",
      "Epoch 11770, Loss: 0.0738938755\n",
      "Epoch 11780, Loss: 0.0731211095\n",
      "Epoch 11790, Loss: 0.0743850350\n",
      "Epoch 11800, Loss: 0.0749152000\n",
      "Epoch 11810, Loss: 0.0735498495\n",
      "Epoch 11820, Loss: 0.0739709857\n",
      "Epoch 11830, Loss: 0.0743585922\n",
      "Epoch 11840, Loss: 0.0752008845\n",
      "Epoch 11850, Loss: 0.0727583320\n",
      "Epoch 11860, Loss: 0.0770317176\n",
      "Epoch 11870, Loss: 0.0746560007\n",
      "Epoch 11880, Loss: 0.0757198374\n",
      "Epoch 11890, Loss: 0.0729076432\n",
      "Epoch 11900, Loss: 0.0738954498\n",
      "Epoch 11910, Loss: 0.0731478325\n",
      "Epoch 11920, Loss: 0.0749618275\n",
      "Epoch 11930, Loss: 0.0738341223\n",
      "Epoch 11940, Loss: 0.0714468588\n",
      "Epoch 11950, Loss: 0.0730329427\n",
      "Epoch 11960, Loss: 0.0733347620\n",
      "Epoch 11970, Loss: 0.0722959903\n",
      "Epoch 11980, Loss: 0.0756376682\n",
      "Epoch 11990, Loss: 0.0761942436\n",
      "Epoch 12000, Loss: 0.0725136162\n",
      "Epoch 12010, Loss: 0.0739250299\n",
      "Epoch 12020, Loss: 0.0748088340\n",
      "Epoch 12030, Loss: 0.0743174954\n",
      "Epoch 12040, Loss: 0.0717275561\n",
      "Epoch 12050, Loss: 0.0740699498\n",
      "Epoch 12060, Loss: 0.0749023393\n",
      "Epoch 12070, Loss: 0.0727033207\n",
      "Epoch 12080, Loss: 0.0721055203\n",
      "Epoch 12090, Loss: 0.0740586069\n",
      "Epoch 12100, Loss: 0.0727308692\n",
      "Epoch 12110, Loss: 0.0727983190\n",
      "Epoch 12120, Loss: 0.0770442894\n",
      "Epoch 12130, Loss: 0.0732039118\n",
      "Epoch 12140, Loss: 0.0725049235\n",
      "Epoch 12150, Loss: 0.0723879188\n",
      "Epoch 12160, Loss: 0.0725835440\n",
      "Epoch 12170, Loss: 0.0746888547\n",
      "Epoch 12180, Loss: 0.0739736584\n",
      "Epoch 12190, Loss: 0.0734802453\n",
      "Epoch 12200, Loss: 0.0728671695\n",
      "Epoch 12210, Loss: 0.0769140172\n",
      "Epoch 12220, Loss: 0.0736254425\n",
      "Epoch 12230, Loss: 0.0741072232\n",
      "Epoch 12240, Loss: 0.0770845251\n",
      "Epoch 12250, Loss: 0.0723624204\n",
      "Epoch 12260, Loss: 0.0760385819\n",
      "Epoch 12270, Loss: 0.0730261220\n",
      "Epoch 12280, Loss: 0.0787644298\n",
      "Epoch 12290, Loss: 0.0743733776\n",
      "Epoch 12300, Loss: 0.0723425286\n",
      "Epoch 12310, Loss: 0.0720173930\n",
      "Epoch 12320, Loss: 0.0728485900\n",
      "Epoch 12330, Loss: 0.0720374301\n",
      "Epoch 12340, Loss: 0.0734899354\n",
      "Epoch 12350, Loss: 0.0753504805\n",
      "Epoch 12360, Loss: 0.0726515788\n",
      "Epoch 12370, Loss: 0.0756693064\n",
      "Epoch 12380, Loss: 0.0719978334\n",
      "Epoch 12390, Loss: 0.0714416689\n",
      "Epoch 12400, Loss: 0.0714669790\n",
      "Epoch 12410, Loss: 0.0729025733\n",
      "Epoch 12420, Loss: 0.0727961494\n",
      "Epoch 12430, Loss: 0.0749456444\n",
      "Epoch 12440, Loss: 0.0721844967\n",
      "Epoch 12450, Loss: 0.0744582805\n",
      "Epoch 12460, Loss: 0.0753317139\n",
      "Epoch 12470, Loss: 0.0731154694\n",
      "Epoch 12480, Loss: 0.0744911198\n",
      "Epoch 12490, Loss: 0.0740174255\n",
      "Epoch 12500, Loss: 0.0741701823\n",
      "Epoch 12510, Loss: 0.0756424890\n",
      "Epoch 12520, Loss: 0.0720449540\n",
      "Epoch 12530, Loss: 0.0722744414\n",
      "Epoch 12540, Loss: 0.0733632480\n",
      "Epoch 12550, Loss: 0.0740180708\n",
      "Epoch 12560, Loss: 0.0731489423\n",
      "Epoch 12570, Loss: 0.0715706474\n",
      "Epoch 12580, Loss: 0.0726160339\n",
      "Epoch 12590, Loss: 0.0715532056\n",
      "Epoch 12600, Loss: 0.0759438384\n",
      "Epoch 12610, Loss: 0.0738566755\n",
      "Epoch 12620, Loss: 0.0742601426\n",
      "Epoch 12630, Loss: 0.0751788226\n",
      "Epoch 12640, Loss: 0.0748116133\n",
      "Epoch 12650, Loss: 0.0742911752\n",
      "Epoch 12660, Loss: 0.0722524160\n",
      "Epoch 12670, Loss: 0.0721027467\n",
      "Epoch 12680, Loss: 0.0718862610\n",
      "Epoch 12690, Loss: 0.0735981810\n",
      "Epoch 12700, Loss: 0.0723460291\n",
      "Epoch 12710, Loss: 0.0725943147\n",
      "Epoch 12720, Loss: 0.0722684605\n",
      "Epoch 12730, Loss: 0.0734371026\n",
      "Epoch 12740, Loss: 0.0711498039\n",
      "Epoch 12750, Loss: 0.0725178806\n",
      "Epoch 12760, Loss: 0.0728257917\n",
      "Epoch 12770, Loss: 0.0730893299\n",
      "Epoch 12780, Loss: 0.0729926574\n",
      "Epoch 12790, Loss: 0.0743205272\n",
      "Epoch 12800, Loss: 0.0723186116\n",
      "Epoch 12810, Loss: 0.0717790158\n",
      "Epoch 12820, Loss: 0.0740688190\n",
      "Epoch 12830, Loss: 0.0741083935\n",
      "Epoch 12840, Loss: 0.0732314933\n",
      "Epoch 12850, Loss: 0.0743457212\n",
      "Epoch 12860, Loss: 0.0722201101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================\n",
    "# 1️⃣ Technical Indicator Functions\n",
    "# ============================\n",
    "def compute_RSI(data, window=14):\n",
    "    delta = np.diff(data)\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "\n",
    "    avg_gain = np.convolve(gain, np.ones(window)/window, mode='valid')\n",
    "    avg_loss = np.convolve(loss, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    rs = avg_gain / (avg_loss + 1e-9)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return np.concatenate((np.full(window, np.nan), rsi))\n",
    "\n",
    "def compute_MACD(data, short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = pd.Series(data).ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = pd.Series(data).ewm(span=long_window, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "    \n",
    "    return macd.values, signal.values\n",
    "\n",
    "def compute_Bollinger_Bands(data, window=20, num_std=2):\n",
    "    rolling_mean = pd.Series(data).rolling(window=window).mean()\n",
    "    rolling_std = pd.Series(data).rolling(window=window).std()\n",
    "    upper_band = rolling_mean + (num_std * rolling_std)\n",
    "    lower_band = rolling_mean - (num_std * rolling_std)\n",
    "    \n",
    "    return rolling_mean.values, upper_band.values, lower_band.values\n",
    "\n",
    "# ============================\n",
    "# 2️⃣ Data Preparation\n",
    "# ============================\n",
    "SEQ_LEN = 5\n",
    "FEATURES = 8  # OHLCV + RSI + MACD + Bollinger Bands\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 50000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Simulated dataset (Replace with real data)\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(1000, 5)  # Simulating 1000 candlesticks (OHLCV)\n",
    "\n",
    "# Compute Indicators\n",
    "close_prices = data[:, 3]\n",
    "\n",
    "rsi = compute_RSI(close_prices)\n",
    "macd, macd_signal = compute_MACD(close_prices)\n",
    "bb_middle, bb_upper, bb_lower = compute_Bollinger_Bands(close_prices)\n",
    "\n",
    "# Align and merge features\n",
    "data = np.column_stack((data, rsi, macd, bb_middle))\n",
    "data = data[30:]  # Remove first 30 rows (due to indicator calculations)\n",
    "\n",
    "# Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert data into sequences\n",
    "X, y = [], []\n",
    "for i in range(len(data) - SEQ_LEN):\n",
    "    X.append(data[i:i + SEQ_LEN])\n",
    "    y.append(data[i + SEQ_LEN][3])  # Predict next Close price\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split Data (Train: 80%, Test: 20%)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Convert to PyTorch tensors & move to GPU\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ============================\n",
    "# 3️⃣ LSTM Model (CUDA)\n",
    "# ============================\n",
    "class CandlestickLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CandlestickLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=FEATURES, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, 1)  # Predict next Close price\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Take last time step's output\n",
    "\n",
    "# Initialize Model, Loss, Optimizer (Move to GPU)\n",
    "model = CandlestickLSTM().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ============================\n",
    "# 4️⃣ Training Loop (CUDA)\n",
    "# ============================\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {avg_loss:.10f}')\n",
    "\n",
    "# ============================\n",
    "# 5️⃣ Evaluation on Test Data (CUDA)\n",
    "# ============================\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = model(X_batch)\n",
    "        y_true.extend(y_batch.cpu().numpy().flatten())\n",
    "        y_pred.extend(predictions.cpu().numpy().flatten())\n",
    "\n",
    "# Convert Predictions Back to Original Scale\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Plot Predictions vs. Actual\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_true, label=\"Actual Close Price\", color='blue', alpha=0.7)\n",
    "plt.plot(y_pred, label=\"Predicted Close Price\", color='red', linestyle=\"dashed\", alpha=0.8)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.title(\"Predicted vs. Actual Close Price (CUDA Optimized)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 6️⃣ Model Performance Metrics\n",
    "# ============================\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"📌 Model Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}\")\n",
    "\n",
    "# ============================\n",
    "# 7️⃣ Prediction Function (CUDA)\n",
    "# ============================\n",
    "def predict_next_close(model, last_5_candles):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_data = torch.tensor(last_5_candles, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        return model(input_data).cpu().item()\n",
    "\n",
    "# Example Prediction\n",
    "sample_input = data[-5:]  # Last 5 candlesticks\n",
    "predicted_close = predict_next_close(model, sample_input)\n",
    "print(\"Predicted Next Close Price:\", predicted_close)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
